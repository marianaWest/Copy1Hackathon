{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS3XunZ1JaAs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import files\n",
        "import requests\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6D1OlC3RsJR"
      },
      "outputs": [],
      "source": [
        "# Download the JSON zip file from GitHub\n",
        "zip_url = \"https://github.com/rae-drt/Copy1Hackathon/raw/refs/heads/main/Metadata/COPY1_catalogue.zip\"\n",
        "zip_filename = \"COPY_1_processed_json.zip\"\n",
        "\n",
        "print(f\"Downloading {zip_filename}...\")\n",
        "response = requests.get(zip_url)\n",
        "with open(zip_filename, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# Extract the zip file\n",
        "extract_dir = \"extracted_json_data/\" # Extract to the parent directory\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Extracting {zip_filename} to {extract_dir}...\")\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# Read each JSON file in the extracted directory and append its content to empty list\n",
        "data_list = []\n",
        "json_files_dir = os.path.join(extract_dir, \"FinalData\") # Look for files in the nested FinalData directory\n",
        "print(f\"Reading JSON files from {json_files_dir}...\")\n",
        "file_count = 0\n",
        "# Check if the json_files_dir exists before listing files\n",
        "if os.path.exists(json_files_dir):\n",
        "    for filename in os.listdir(json_files_dir):\n",
        "        if filename.startswith('reject'):\n",
        "            continue\n",
        "        if filename.endswith(\".json\"):\n",
        "            filepath = os.path.join(json_files_dir, filename)\n",
        "            print(f\"Reading file: {filepath}\")\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, list):\n",
        "                        data_list.extend(data)\n",
        "                    else:\n",
        "                        data_list.append(data)\n",
        "                file_count += 1\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON in file {filepath}: {e}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {json_files_dir}\")\n",
        "\n",
        "print(f\"Finished reading {file_count} JSON files.\")\n",
        "print(f\"Total items in data_list: {len(data_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ca5819"
      },
      "source": [
        "# Extract the inner dictionaries from data_list\n",
        "processed_data = []\n",
        "for item in data_list:\n",
        "    # Assuming each item in data_list is a dictionary with one key and a nested dictionary as value\n",
        "    # We want to extract the nested dictionary\n",
        "    if isinstance(item, dict):\n",
        "        for key, value in item.items():\n",
        "            if isinstance(value, dict):\n",
        "                processed_data.append(value)\n",
        "            else:\n",
        "                print(f\"Warning: Value for key '{key}' is not a dictionary: {value}\")\n",
        "    else:\n",
        "        print(f\"Warning: Item in data_list is not a dictionary: {item}\")\n",
        "\n",
        "\n",
        "# Convert the processed data (list of inner dictionaries) to DataFrame\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Normalize nested JSON columns, handling potential errors\n",
        "try:\n",
        "    description_fields_df = pd.json_normalize(df['DescriptionFields'])\n",
        "    scope_content_df = pd.json_normalize(df['scopeContent'])\n",
        "\n",
        "    # Concatenate the normalized dataframes and drop the original nested columns\n",
        "    df = pd.concat([df.drop(['DescriptionFields', 'scopeContent'], axis=1), description_fields_df, scope_content_df], axis=1)\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError during normalization: {e}. Skipping normalization for these columns.\")\n",
        "    # If KeyError occurs, keep the original df without normalization of missing columns\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during normalization: {e}. Skipping normalization.\")\n",
        "\n",
        "\n",
        "# Display the first few rows of the resulting dataframe\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split a column by the first comma and create two new columns and strip whitespace\n",
        "def split_column_by_first_comma(df, column_name):\n",
        "    if column_name in df.columns:\n",
        "        split_data = df[column_name].astype(str).str.split(',', n=1, expand=True)\n",
        "        df[f'{column_name}_name'] = split_data[0].str.strip() # Apply strip to the name part\n",
        "        # Check if the second part exists before trying to access it and strip it\n",
        "        if split_data.shape[1] > 1:\n",
        "            df[f'{column_name}_address'] = split_data[1].str.strip() # Apply strip to the address part\n",
        "            # Remove trailing full stop if it exists (after stripping)\n",
        "            df[f'{column_name}_address'] = df[f'{column_name}_address'].astype(str).str.rstrip('.')\n",
        "        else:\n",
        "            df[f'{column_name}_address'] = None # Or any other placeholder for missing data\n",
        "    return df\n",
        "\n",
        "# Split 'copyright owner' column\n",
        "df = split_column_by_first_comma(df, 'CopyrightOwner')\n",
        "\n",
        "# Split 'copyrightauthor' column\n",
        "df = split_column_by_first_comma(df, 'CopyrightAuthor')"
      ],
      "metadata": {
        "id": "aeQo4xRWcUv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7vw-pr4tYvM"
      },
      "outputs": [],
      "source": [
        "# Reorder columns\n",
        "original_columns = df.columns.tolist()\n",
        "reordered_columns = []\n",
        "for col in original_columns:\n",
        "    reordered_columns.append(col)\n",
        "    if col == 'CopyrightOwner':\n",
        "        reordered_columns.append('CopyrightOwner_name')\n",
        "        reordered_columns.append('CopyrightOwner_address')\n",
        "    elif col == 'CopyrightAuthor':\n",
        "        reordered_columns.append('CopyrightAuthor_name')\n",
        "        reordered_columns.append('CopyrightAuthor_address')\n",
        "\n",
        "# Remove duplicates caused by adding the new columns already\n",
        "reordered_columns = [col for col in reordered_columns if col in original_columns or col.endswith('_name') or col.endswith('_address')]\n",
        "reordered_columns = list(dict.fromkeys(reordered_columns)) # Remove duplicates while preserving order\n",
        "\n",
        "# Filter out the original name and address columns from their original positions\n",
        "reordered_columns = [col for col in reordered_columns if col not in ['CopyrightOwner_name', 'CopyrightOwner_address', 'CopyrightAuthor_name', 'CopyrightAuthor_address']]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the name and address columns after the original columns\n",
        "final_columns = []\n",
        "for col in df.columns:\n",
        "    final_columns.append(col)\n",
        "    if col == 'CopyrightOwner':\n",
        "        final_columns.append('CopyrightOwner_name')\n",
        "        final_columns.append('CopyrightOwner_address')\n",
        "    elif col == 'CopyrightAuthor':\n",
        "        final_columns.append('CopyrightAuthor_name')\n",
        "        final_columns.append('CopyrightAuthor_address')"
      ],
      "metadata": {
        "id": "J4Appc9EdEjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates from the final list\n",
        "final_columns = list(dict.fromkeys(final_columns))"
      ],
      "metadata": {
        "id": "11N6GGoGdHCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns in the desired order\n",
        "df = df[final_columns]"
      ],
      "metadata": {
        "id": "8ZfWHYI5dIHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove columns starting with 'C' followed by a number, and specific columns\n",
        "columns_to_drop = [col for col in df.columns if pd.Series(col).str.match(r'^C\\d+').any()]\n",
        "columns_to_drop.extend(['ephemera', 'placeNames']) # Add 'ephemera' and 'placeNames' to the list\n",
        "\n",
        "# Ensure no duplicate column names are in the drop list\n",
        "columns_to_drop = list(set(columns_to_drop))\n",
        "\n",
        "df = df.drop(columns=columns_to_drop, errors='ignore') # Use errors='ignore' to avoid error if columns don't exist\n",
        "\n",
        "# Rename 'description' column to 'scopecontent'\n",
        "if 'description' in df.columns:\n",
        "    df = df.rename(columns={'description': 'scopecontent'})\n",
        "    print(\"Renamed 'description' column to 'scopecontent'\")\n",
        "else:\n",
        "    print(\"'description' column not found.\")\n",
        "\n",
        "\n",
        "# Display the first few rows of the modified dataframe to confirm\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "vfNWw4c2gySE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7b5ab7d"
      },
      "source": [
        "from ipywidgets import IntRangeSlider, interact\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "\n",
        "# Assuming your data has a 'coveringFromDate' column from which we can determine the year range\n",
        "# Get the minimum and maximum years from the dataframe, handling potential non-numeric values\n",
        "# Convert to numeric, coerce errors to NaN, drop NaNs, convert to string, slice, convert to int\n",
        "numeric_dates = pd.to_numeric(df['coveringFromDate'], errors='coerce').dropna().astype(int).astype(str).str[:4]\n",
        "min_year = int(numeric_dates.min()) if not numeric_dates.empty else 1900 # Default if no valid years\n",
        "max_year = int(numeric_dates.max()) if not numeric_dates.empty else 2000 # Default if no valid years\n",
        "\n",
        "# Set the desired minimum and maximum years for the slider\n",
        "slider_min_year = 1860\n",
        "slider_max_year = 1912\n",
        "\n",
        "# Create a range slider for the year range\n",
        "year_range_slider = IntRangeSlider(\n",
        "    value=[max(min_year, slider_min_year), min(max_year, slider_max_year)], # Set initial value within the desired range\n",
        "    min=slider_min_year,\n",
        "    max=slider_max_year,\n",
        "    step=1,\n",
        "    description='Select Year Range:',\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='d'\n",
        ")\n",
        "\n",
        "display(year_range_slider)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjrK0Pv8vb4o"
      },
      "source": [
        "# Get the selected start and end years from the slider\n",
        "start_year = year_range_slider.value[0]\n",
        "end_year = year_range_slider.value[1]\n",
        "\n",
        "# Convert 'coveringFromDate' to numeric, coerce errors to NaN, and then to string before slicing\n",
        "covering_from_year_str = pd.to_numeric(df['coveringFromDate'], errors='coerce').astype('Int64').astype(str).str[:4]\n",
        "\n",
        "# Create a boolean mask for rows where the extracted year string is a digit\n",
        "is_digit_mask = covering_from_year_str.str.isdigit()\n",
        "\n",
        "# Apply the digit mask first, then convert to int for comparison\n",
        "# This ensures that we only attempt to convert strings that are actually digits\n",
        "mask = is_digit_mask & (covering_from_year_str[is_digit_mask].astype(int).between(start_year, end_year))\n",
        "\n",
        "# Apply the mask to the DataFrame, reindexing to handle the boolean mask\n",
        "df_filtered = df[mask.reindex(df.index, fill_value=False)]\n",
        "\n",
        "print(f\"DataFrame filtered for years between {start_year} and {end_year}.\")\n",
        "display(df_filtered.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4299df3f"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "output_csv_file = f\"COPY 1_json_combined_split_{start_year}_{end_year}.csv\"\n",
        "df_filtered.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to '{output_csv_file}'\")\n",
        "\n",
        "# Download the CSV file\n",
        "from google.colab import files\n",
        "try:\n",
        "  files.download(output_csv_file)\n",
        "except Exception as e:\n",
        "  print(f\"Error downloading file: {e}\")\n",
        "  print(\"Please ensure the file path is correct.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}